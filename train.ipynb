{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1bc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import nbimporter\n",
    "import utils\n",
    "import carvana\n",
    "import dataset\n",
    "import unet\n",
    "# from unet import YouNet, UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c1769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "NUM_WORKERS = config[\"hyperparams\"][\"NUM_WORKERS\"]\n",
    "BATCH_SIZE = config[\"hyperparams\"][\"BATCH_SIZE\"]\n",
    "NUM_EPOCHS = config[\"hyperparams\"][\"NUM_EPOCHS\"]\n",
    "LEARNING_RATE = float(config[\"hyperparams\"][\"LEARNING_RATE\"])\n",
    "WEIGHT_DECAY = float(config[\"hyperparams\"][\"WEIGHT_DECAY\"])\n",
    "IMAGE_HEIGHT = config[\"hyperparams\"][\"IMAGE_HEIGHT\"]\n",
    "IMAGE_WIDTH = config[\"hyperparams\"][\"IMAGE_WIDTH\"]\n",
    "CROP_SIZE = config[\"hyperparams\"][\"CROP_SIZE\"]\n",
    "\n",
    "device_str = config.get(\"device\", \"auto\")\n",
    "if device_str == \"auto\":\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    DEVICE = torch.device(device_str)\n",
    "    if DEVICE.type == \"cuda\" and not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available but 'cuda' was specified.\")\n",
    "\n",
    "\n",
    "DATA_DIR = config[\"paths\"][\"DATA_DIR\"]\n",
    "OUTPUT_DIR = config[\"paths\"][\"OUTPUT_DIR\"]\n",
    "WEIGHTS_DIR = config[\"paths\"][\"WEIGHTS_DIR\"]\n",
    "\n",
    "# LOSS_WEIGHTS_DIR = \"'C:/Users/Hayden/Desktop/class_weights.npy\"\n",
    "# class_weights = np.load(LOSS_WEIGHTS_DIR)\n",
    "# LOSS_WEIGHTS_TENSOR = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "VOC_COLORMAP = config[\"voc_dataset\"][\"VOC_COLORMAP\"]\n",
    "VOC_CLASSES = config[\"voc_dataset\"][\"VOC_CLASSES\"]\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57c2850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, image):\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image.to(device)).argmax(1, keepdim=True)\n",
    "    return prediction\n",
    "\n",
    "def prediction_to_image(prediction, device):\n",
    "    VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "    color_map = torch.tensor(VOC_COLORMAP, device=device, dtype=torch.uint8)\n",
    "    X = prediction.squeeze(1).long()\n",
    "    rgb = color_map[X, :].permute(0, 3, 1, 2)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050d45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(prediction, label):\n",
    "    if len(prediction.shape) > 1 and prediction.shape[1] > 1:\n",
    "        prediction = torch.argmax(prediction, axis=1)\n",
    "    label = label.squeeze(1)\n",
    "    cmp = prediction.type(label.dtype) == label\n",
    "    return float(torch.sum(cmp.type(label.dtype)))\n",
    "\n",
    "def accuracy(prediction, label):\n",
    "    \"\"\"\n",
    "    Example Usage:\n",
    "        a = torch.tensor([1, 2, 3, 4])\n",
    "        b = torch.tensor([1, 0, 3, 5])\n",
    "        accuracy(a, b) -> torch.tensor([1, 0, 1, 0])\n",
    "    \"\"\"\n",
    "    num_matches = (prediction == label).sum().item()\n",
    "    print(num_matches)\n",
    "    return num_matches\n",
    "\n",
    "def compute_accuracy(pred, label):\n",
    "    pred = pred.argmax(1) if pred.dim() > 1 and pred.size(1) > 1 else pred\n",
    "    label = label.squeeze(1) if label.dim() == 4 else label\n",
    "    correct = (pred == label).sum().item()\n",
    "    total = label.numel()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf52982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(loader, model, optimizer, loss_fn, scaler):\n",
    "    model.train()\n",
    "    for batch_i, datapoint in enumerate(loader):\n",
    "        # print(f'BATCH {batch_i}')\n",
    "        optimizer.zero_grad()\n",
    "        X = datapoint['images'].to(DEVICE)\n",
    "        y = datapoint['masks'].to(DEVICE)\n",
    "        # X = X.to(DEVICE)\n",
    "        # y = y.to(DEVICE)\n",
    "\n",
    "        # Forward propagation\n",
    "        # with torch.autocast(device_type=DEVICE, dtype=torch.float16):\n",
    "        # with autocast():\n",
    "        print('X.shape=', X.shape)\n",
    "        y_hat = model(X)\n",
    "        if y.min() < 0 or y.max() >= y_hat.shape[1]:\n",
    "            print(f\"âŒ Invalid label value in batch: min={y.min().item()}, max={y.max().item()}\")\n",
    "            raise ValueError(\"Label contains out-of-bound class indices.\")\n",
    "        loss = loss_fn(y_hat, y.squeeze(1).long())\n",
    "        \n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "\n",
    "        train_loss_sum = loss.sum()\n",
    "        train_accuracy_sum = prediction_accuracy(y_hat, y)\n",
    "\n",
    "        return train_loss_sum, train_accuracy_sum\n",
    "\n",
    "        # total_loss += loss.item()\n",
    "        # total_correct += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "        # return total_loss / len(train_loader), total_correct / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c7ee71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, transforms, batch_size):\n",
    "def train():\n",
    "    model = unet.YouNet(in_channels=3, out_channels=21).to(DEVICE)\n",
    "    # model = unet.UNet(in_channels=3, out_channels=21).to(DEVICE)\n",
    "    train_transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    validation_transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    test_transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    train_loader = dataset.get_data_loader(DATA_DIR, train_transform, train_transform, CROP_SIZE, 'train', BATCH_SIZE)\n",
    "    validation_loader = dataset.get_data_loader(DATA_DIR, train_transform, train_transform, CROP_SIZE, 'val', BATCH_SIZE)\n",
    "    \n",
    "    # Initialize weights\n",
    "    # def init_weights(module):\n",
    "    #     if type(module) in [torch.nn.Linear, torch.nn.Conv2d]:\n",
    "    #         torch.nn.init.normal_(module.weight, std=0.01)\n",
    "    # model.apply(init_weights)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS+1):\n",
    "        print(f'EPOCH {epoch}:')\n",
    "\n",
    "        model.train()\n",
    "        loss, accuracy = train_batch(train_loader, model, optimizer, loss_fn, scaler)\n",
    "        print(f'Training loss: {loss}')#\\nPrediction accuracy: {accuracy}')\n",
    "\n",
    "        # checkpoint = {\n",
    "        #     'model': model.state_dict(),\n",
    "        #     'optimizer': optimizer.state_dict()\n",
    "        # }\n",
    "        # utils.save_checkpoint(checkpoint)\n",
    "\n",
    "        # TODO: Complete the predictions accuracy function and save predictions to disc.\n",
    "        # predictions accuracy\n",
    "        \n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "                model.eval()\n",
    "                elements = None\n",
    "                for idx, datapoint in enumerate(validation_loader):\n",
    "                    val_images = datapoint['images'].to(DEVICE)\n",
    "                    val_masks = datapoint['masks'].to(DEVICE)\n",
    "                    # val_images = val_images.to(DEVICE)\n",
    "                    # val_masks = val_masks.to(DEVICE)\n",
    "                    \n",
    "                    prediction = predict(model, DEVICE, val_images)\n",
    "                    if elements is None:\n",
    "                        elements = torch.unique(prediction)\n",
    "                    else:\n",
    "                        elements = torch.cat((elements, torch.unique(prediction)), dim=0)\n",
    "\n",
    "                    val_correct += (prediction == val_masks).sum().item()\n",
    "                    val_total += val_masks.numel()\n",
    "                    # valid_acc = prediction_accuracy(prediction, val_masks)\n",
    "                    # print(f'Validation accuracy:  {valid_acc}')\n",
    "\n",
    "                    if idx >= 50:\n",
    "                        break\n",
    "                    if epoch % 5 == 0:\n",
    "                        prediction_image = prediction_to_image(prediction, DEVICE)\n",
    "\n",
    "                        image_path = os.path.join(OUTPUT_DIR, f'images_{epoch}')\n",
    "                        if not os.path.exists(image_path):\n",
    "                            os.makedirs(image_path, exist_ok=True)\n",
    "                        \n",
    "                        for pred_image, val_image, val_mask in zip(prediction_image, val_images, val_masks):\n",
    "                            pred_image = to_pil_image(pred_image.cpu())\n",
    "                            pred_image.save(os.path.join(image_path, f'{idx}_prediction.png'))\n",
    "\n",
    "                            val_image = dataset.denormalize(val_image.cpu())\n",
    "                            val_image = to_pil_image(val_image.cpu())\n",
    "                            val_image.save(os.path.join(image_path, f'{idx}_image.png'))\n",
    "\n",
    "                            val_mask = dataset.mask_to_image(val_mask.cpu(), VOC_COLORMAP)\n",
    "                            val_mask = to_pil_image(val_mask.squeeze(0).cpu())\n",
    "                            val_mask.save(os.path.join(image_path, f'{idx}_mask.png'))\n",
    "                \n",
    "                valid_acc = val_correct / val_total\n",
    "                print(f'Validation accuracy:  {valid_acc}')\n",
    "                print(torch.unique(elements))\n",
    "    # for test_image, test_mask in test_loader:\n",
    "    #     prediction = utils.prediction_to_image(utils.predict(test_loader, model, DEVICE, test_image), DEVICE)\n",
    "    #     test_acc = utils.prediction_accuracy(prediction, test_mask)\n",
    "    #     print(f'Test Accuracy: {test_acc}')\n",
    "\n",
    "        \n",
    "        \n",
    "        # utils.save_predictions(validation_loader, model, data_dir='predictions/', device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879aa878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels=3, out_channels=64\n",
      "in_channels=64, out_channels=128\n",
      "in_channels=128, out_channels=256\n",
      "in_channels=256, out_channels=512\n",
      "in_channels=512, out_channels=1024\n",
      "in_channels=1024, out_channels=512\n",
      "in_channels=512, out_channels=256\n",
      "in_channels=256, out_channels=128\n",
      "in_channels=128, out_channels=64\n",
      "EPOCH 0:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0407819747924805\n",
      "Validation accuracy:  0.0023651123046875\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 1:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.045356273651123\n",
      "Validation accuracy:  0.01544189453125\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 2:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.044745445251465\n",
      "Validation accuracy:  0.011160757027420342\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 3:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0473082065582275\n",
      "Validation accuracy:  0.007504631491268382\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 4:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0406699180603027\n",
      "Validation accuracy:  0.02940967036228554\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 5:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.045560598373413\n",
      "Validation accuracy:  0.024234846526501225\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 6:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0451314449310303\n",
      "Validation accuracy:  0.011089250153186275\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 7:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0452473163604736\n",
      "Validation accuracy:  0.015777587890625\n",
      "tensor([ 3,  4,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 8:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0439915657043457\n",
      "Validation accuracy:  0.01431573606004902\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 9:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0487468242645264\n",
      "Validation accuracy:  0.02114269780177696\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 10:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0435054302215576\n",
      "Validation accuracy:  0.005914725509344363\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 11:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.045754909515381\n",
      "Validation accuracy:  0.016598271388633578\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 12:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.04671049118042\n",
      "Validation accuracy:  0.015415565640318627\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 13:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.044231653213501\n",
      "Validation accuracy:  0.0055757410386029415\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 14:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: nan\n",
      "Validation accuracy:  0.011160757027420342\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 15:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.046271562576294\n",
      "Validation accuracy:  0.03651039273131127\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 16:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0482871532440186\n",
      "Validation accuracy:  0.004207835477941176\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 17:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.038870334625244\n",
      "Validation accuracy:  0.009419459922640932\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 18:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0421741008758545\n",
      "Validation accuracy:  0.004424151252297794\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 19:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.041818857192993\n",
      "Validation accuracy:  0.004169239717371324\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 20:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.052473783493042\n",
      "Validation accuracy:  0.011739394244025736\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 21:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.043513536453247\n",
      "Validation accuracy:  0.027998980353860295\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 22:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.051722764968872\n",
      "Validation accuracy:  0.009971169864430147\n",
      "tensor([ 3,  9, 11, 14], device='cuda:0')\n",
      "EPOCH 23:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0469796657562256\n",
      "Validation accuracy:  0.020801918179381127\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 24:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0388331413269043\n",
      "Validation accuracy:  0.011403102500765932\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 25:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0451581478118896\n",
      "Validation accuracy:  0.023391125248927697\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 26:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0523126125335693\n",
      "Validation accuracy:  0.012758142807904412\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 27:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.045785665512085\n",
      "Validation accuracy:  0.023009356330422795\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 28:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0489015579223633\n",
      "Validation accuracy:  0.019693412032781864\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 29:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0466361045837402\n",
      "Validation accuracy:  0.02995479808134191\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n",
      "EPOCH 30:\n",
      "X.shape= torch.Size([1, 3, 256, 256])\n",
      "Training loss: 3.0429093837738037\n",
      "Validation accuracy:  0.008792353611366422\n",
      "tensor([ 3,  9, 11, 14, 15], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
