{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8751583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "import nbimporter\n",
    "from dataset import VocDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a60ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_block(in_channels, out_channels):\n",
    "    print(f'in_channels={in_channels}, out_channels={out_channels}')\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb251dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    skip_connections = []\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encode=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.encode = encode\n",
    "        self.conv = build_conv_block(in_channels=self.in_channels, out_channels=self.out_channels)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        print(f'X.shape input={X.shape}')\n",
    "        if self.encode:\n",
    "            X = self.conv(X)\n",
    "            print(f'X.shape output={X.shape}')\n",
    "            self.skip_connections.append(X)\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)(X)\n",
    "        else:\n",
    "            X = nn.ConvTranspose2d(in_channels=self.in_channels, out_channels=self.in_channels//2, kernel_size=2, stride=2)(X)\n",
    "            # if X[0].shape[0] <= 256:\n",
    "            #     print(X.shape)\n",
    "            #     print(self.skip_connections[-1].shape)\n",
    "            X = torch.cat((X, self.skip_connections.pop()), dim=1)\n",
    "            print(f'X.shape output={X.shape}')\n",
    "            return self.conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023ff8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouNet(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=64):\n",
    "        super(YouNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # The down-sampling layers\n",
    "        self.contractive_path = nn.ModuleDict({\n",
    "            'encode0': ConvBlock(in_channels=self.in_channels, out_channels=64, encode=True),\n",
    "            'encode1': ConvBlock(in_channels=64, out_channels=128, encode=True),\n",
    "            'encode2': ConvBlock(in_channels=128, out_channels=256, encode=True),\n",
    "            'encode3': ConvBlock(in_channels=256, out_channels=512, encode=True),\n",
    "        })\n",
    "\n",
    "        # The bottleneck\n",
    "        self.trough = build_conv_block(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # The up-sampling layers\n",
    "        # in_channels takes input from previous layer and skip connections\n",
    "        self.expansive_path = nn.ModuleDict({\n",
    "            'decode3': ConvBlock(in_channels=512*2, out_channels=512, encode=False),\n",
    "            'decode2': ConvBlock(in_channels=256*2, out_channels=256, encode=False),\n",
    "            'decode1': ConvBlock(in_channels=128*2, out_channels=128, encode=False),\n",
    "            'decode0': ConvBlock(in_channels=64*2, out_channels=64, encode=False)\n",
    "        })\n",
    "\n",
    "        # The prediction layer\n",
    "        self.final = nn.Conv2d(in_channels=64, out_channels=self.out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Train the contractive path\n",
    "        for conv_block in self.contractive_path:\n",
    "            # print(f'X.shape input={X.shape}')\n",
    "            X = self.contractive_path[conv_block](X)\n",
    "            # print(f'X.shape output={X.shape}')\n",
    "        \n",
    "        # Train the trough\n",
    "        X = self.trough(X)\n",
    "        \n",
    "        # Train the expansive path\n",
    "        for conv_block in self.expansive_path:\n",
    "            X = self.expansive_path[conv_block](X)\n",
    "        \n",
    "        return self.final(X)\n",
    "    \n",
    "    def print_hook_shape(self, module, input, output):\n",
    "        '''Prints the input and output tensor of a given layer. Used by YouNet.print_forward_hooks'''\n",
    "        print(f'{module.__class__.__name__}(input shape: {input[0].shape}, output shape: {output.shape})')\n",
    "\n",
    "    def print_forward_hooks(self):\n",
    "        '''Prints the input and output tensors of each layer.'''\n",
    "        for name, layer in self.contractive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.trough.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        for name, layer in self.expansive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.final.register_forward_hook(self.print_hook_shape)\n",
    "    \n",
    "    def print_network(self):\n",
    "        '''Prints the entire network architecture.'''\n",
    "        for name, conv_block in self.contractive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "\n",
    "        print('Layer: bottleneck')\n",
    "        print(self.trough)\n",
    "\n",
    "        for name, conv_block in self.expansive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "        \n",
    "        print('Layer: final\\n', self.final, sep='')\n",
    "\n",
    "# YouNet expects inputs with shape (any_batch_size, any_channel_size, any_even_height, any_even_width)\n",
    "# TODO: Implement padding or shaving of inputs to accept any input height or width (even or odd)\n",
    "# X = torch.randn((32, 3, 320, 480))\n",
    "# net = YouNet(3, 3)\n",
    "\n",
    "# net.print_network()\n",
    "# print()\n",
    "# net.print_forward_hooks()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     preds = net(X)\n",
    "\n",
    "# print()\n",
    "# print('Input shape:', X.shape)\n",
    "# print('Output shape:', preds.shape)\n",
    "# assert preds.shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707f2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n",
      "1426\n",
      "1429\n"
     ]
    }
   ],
   "source": [
    "voc_dir = 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/'\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "crop_size = (256, 256)\n",
    "dataset = 'train'\n",
    "train_set = VocDataset(voc_dir, transform, transform, crop_size, dataset)\n",
    "val_set = VocDataset(voc_dir, transform, transform, crop_size, 'val')\n",
    "test_set = VocDataset(voc_dir, transform, transform, crop_size, 'test')\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70722c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3883, -0.2856, -0.1999,  ...,  0.8447,  0.3823,  0.6392],\n",
       "         [-0.3712, -0.3883, -0.3541,  ...,  0.8789,  0.3823,  0.6563],\n",
       "         [-0.3712, -0.4911, -0.5082,  ...,  0.8447,  0.3652,  0.6392],\n",
       "         ...,\n",
       "         [ 0.6906, -0.3712, -0.3541,  ..., -0.9705, -0.8849, -0.7822],\n",
       "         [-0.1657, -0.1143, -0.2684,  ..., -0.9020, -0.7308, -0.6281],\n",
       "         [-0.0629, -0.0629, -0.2171,  ..., -1.0390, -0.8678, -0.7479]],\n",
       "\n",
       "        [[-0.8277, -0.7227, -0.6527,  ...,  0.3277, -0.3200, -0.1450],\n",
       "         [-0.8102, -0.8277, -0.8102,  ...,  0.3102, -0.3200, -0.1275],\n",
       "         [-0.8102, -0.9328, -0.9678,  ...,  0.2752, -0.3725, -0.1800],\n",
       "         ...,\n",
       "         [-0.3901, -1.6506, -1.5980,  ..., -1.5805, -1.4755, -1.3880],\n",
       "         [-1.5805, -1.4755, -1.5630,  ..., -1.7031, -1.6155, -1.5105],\n",
       "         [-1.4580, -1.4230, -1.5805,  ..., -1.7556, -1.6506, -1.5455]],\n",
       "\n",
       "        [[-1.4907, -1.3861, -1.3164,  ..., -0.8981, -1.3861, -1.1073],\n",
       "         [-1.4384, -1.4907, -1.4733,  ..., -0.8981, -1.3861, -1.0898],\n",
       "         [-1.4384, -1.5604, -1.6302,  ..., -0.9330, -1.4210, -1.1247],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.7173,  ..., -1.7347, -1.6999, -1.6824],\n",
       "         [-1.7696, -1.7870, -1.8044,  ..., -1.7173, -1.6824, -1.6302],\n",
       "         [-1.7522, -1.6999, -1.6999,  ..., -1.8044, -1.7522, -1.6824]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f8db224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([1, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "in_channels=3, out_channels=64\n",
      "in_channels=64, out_channels=128\n",
      "in_channels=128, out_channels=256\n",
      "in_channels=256, out_channels=512\n",
      "in_channels=512, out_channels=1024\n",
      "in_channels=1024, out_channels=512\n",
      "in_channels=512, out_channels=256\n",
      "in_channels=256, out_channels=128\n",
      "in_channels=128, out_channels=64\n",
      "X.shape input=torch.Size([1, 3, 256, 256])\n",
      "X.shape output=torch.Size([1, 64, 256, 256])\n",
      "X.shape input=torch.Size([1, 64, 128, 128])\n",
      "X.shape output=torch.Size([1, 128, 128, 128])\n",
      "X.shape input=torch.Size([1, 128, 64, 64])\n",
      "X.shape output=torch.Size([1, 256, 64, 64])\n",
      "X.shape input=torch.Size([1, 256, 32, 32])\n",
      "X.shape output=torch.Size([1, 512, 32, 32])\n",
      "X.shape input=torch.Size([1, 1024, 16, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m mynet \u001b[38;5;241m=\u001b[39m YouNet(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmynet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hayden\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 42\u001b[0m, in \u001b[0;36mYouNet.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Train the expansive path\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpansive_path:\n\u001b[1;32m---> 42\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpansive_path\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconv_block\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(X)\n",
      "File \u001b[1;32mc:\\Users\\Hayden\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)(X)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvTranspose2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# if X[0].shape[0] <= 256:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#     print(X.shape)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#     print(self.skip_connections[-1].shape)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connections\u001b[38;5;241m.\u001b[39mpop()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hayden\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hayden\\anaconda3\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    951\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    952\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    954\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "image, mask = train_set[0]\n",
    "print(image.shape, mask.shape)\n",
    "image = torch.unsqueeze(image, dim=0)\n",
    "mask = torch.unsqueeze(mask, dim=0)\n",
    "print(image.shape, mask.shape)\n",
    "mynet = YouNet(3, 3).cuda()\n",
    "with torch.no_grad():\n",
    "    pred = mynet(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2269eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[-0.6650, -0.3076, -0.6735,  ..., -1.1186, -0.8151, -1.6791],\n",
      "          [-1.0963, -0.7336, -0.3296,  ..., -0.6975,  0.2084, -0.7240],\n",
      "          [-1.0764, -0.3929, -0.6100,  ..., -1.0809, -0.5923, -0.4833],\n",
      "          ...,\n",
      "          [-0.3296, -0.4373, -0.3289,  ..., -0.9929, -0.8269, -0.6770],\n",
      "          [-0.6967, -0.9322, -0.7660,  ..., -0.8244, -1.0234, -1.1647],\n",
      "          [-0.2097, -0.1567,  0.1230,  ..., -0.1336,  0.0516, -0.0035]],\n",
      "\n",
      "         [[-0.7101, -0.7239, -0.4165,  ..., -0.2840, -0.6845, -0.2599],\n",
      "          [-0.2840, -0.2204, -0.2974,  ...,  0.6209, -0.4773,  0.4224],\n",
      "          [-0.3414, -0.8442, -0.5291,  ..., -0.6757, -0.2749, -0.2343],\n",
      "          ...,\n",
      "          [-0.6379, -0.4814, -0.8504,  ..., -0.4851, -0.4969, -0.0518],\n",
      "          [-0.4699,  0.2723, -0.2632,  ..., -0.1172, -0.5123, -0.7423],\n",
      "          [-0.5320, -0.5711, -0.4483,  ..., -0.2985, -0.7793, -0.4902]],\n",
      "\n",
      "         [[-0.7541, -0.9422, -0.5056,  ..., -0.7995, -0.2589,  0.2776],\n",
      "          [-1.0750, -1.5533, -0.8143,  ..., -0.0455, -0.0927,  1.9799],\n",
      "          [-0.5763, -0.6885, -0.8651,  ..., -0.1824, -0.6269,  1.0102],\n",
      "          ...,\n",
      "          [ 0.5367, -0.4324, -0.0098,  ..., -0.0523, -0.5141, -0.0802],\n",
      "          [-0.3000, -0.8963, -0.6404,  ..., -0.4165, -0.6131, -0.0393],\n",
      "          [-0.0574, -0.6080, -0.1008,  ..., -0.8389, -1.0685, -0.2369]]]])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36105c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
