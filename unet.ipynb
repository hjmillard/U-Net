{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8751583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "import nbimporter\n",
    "from dataset import VocDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_block(in_channels, out_channels):\n",
    "    print(f'in_channels={in_channels}, out_channels={out_channels}')\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        \n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb251dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    skip_connections = []\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encode=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.encode = encode\n",
    "        self.conv = build_conv_block(in_channels=self.in_channels, out_channels=self.out_channels)\n",
    "        if self.encode:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        else:\n",
    "            self.upconv = nn.ConvTranspose2d(in_channels=self.in_channels, out_channels=self.in_channels//2, kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # print(f'X.shape input={X.shape}')\n",
    "        if self.encode:\n",
    "            X = self.conv(X)\n",
    "            # print(f'X.shape output={X.shape}')\n",
    "            self.skip_connections.append(X)\n",
    "            return self.pool(X)\n",
    "        else:\n",
    "            X = self.upconv(X)\n",
    "            # if X[0].shape[0] <= 256:\n",
    "            #     print(X.shape)\n",
    "            #     print(self.skip_connections[-1].shape)\n",
    "            X = torch.cat((X, self.skip_connections.pop()), dim=1)\n",
    "            # print(f'X.shape output={X.shape}')\n",
    "            return self.conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ff8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=21):\n",
    "        super(YouNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # The down-sampling layers\n",
    "        self.contractive_path = nn.ModuleDict({\n",
    "            'encode0': ConvBlock(in_channels=self.in_channels, out_channels=64, encode=True),\n",
    "            'encode1': ConvBlock(in_channels=64, out_channels=128, encode=True),\n",
    "            'encode2': ConvBlock(in_channels=128, out_channels=256, encode=True),\n",
    "            'encode3': ConvBlock(in_channels=256, out_channels=512, encode=True),\n",
    "        })\n",
    "\n",
    "        # The bottleneck\n",
    "        self.trough = build_conv_block(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # The up-sampling layers\n",
    "        # in_channels takes input from previous layer and skip connections\n",
    "        self.expansive_path = nn.ModuleDict({\n",
    "            'decode3': ConvBlock(in_channels=512*2, out_channels=512, encode=False),\n",
    "            'decode2': ConvBlock(in_channels=256*2, out_channels=256, encode=False),\n",
    "            'decode1': ConvBlock(in_channels=128*2, out_channels=128, encode=False),\n",
    "            'decode0': ConvBlock(in_channels=64*2, out_channels=64, encode=False)\n",
    "        })\n",
    "\n",
    "        # The prediction layer\n",
    "        self.final = nn.Conv2d(in_channels=64, out_channels=self.out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        skip_connections = []\n",
    "        # Train the contractive path\n",
    "        for conv_block in self.contractive_path:\n",
    "            # print(f'X.shape input={X.shape}')\n",
    "            X = self.contractive_path[conv_block](X)\n",
    "            # print(f'X.shape output={X.shape}')\n",
    "        \n",
    "        # Train the trough\n",
    "        X = self.trough(X)\n",
    "        \n",
    "        # Train the expansive path\n",
    "        for conv_block in self.expansive_path:\n",
    "            X = self.expansive_path[conv_block](X)\n",
    "        \n",
    "        return self.final(X)\n",
    "    \n",
    "    def print_hook_shape(self, module, input, output):\n",
    "        '''Prints the input and output tensor of a given layer. Used by YouNet.print_forward_hooks'''\n",
    "        print(f'{module.__class__.__name__}(input shape: {input[0].shape}, output shape: {output.shape})')\n",
    "\n",
    "    def print_forward_hooks(self):\n",
    "        '''Prints the input and output tensors of each layer.'''\n",
    "        for name, layer in self.contractive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.trough.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        for name, layer in self.expansive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.final.register_forward_hook(self.print_hook_shape)\n",
    "    \n",
    "    def print_network(self):\n",
    "        '''Prints the entire network architecture.'''\n",
    "        for name, conv_block in self.contractive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "\n",
    "        print('Layer: bottleneck')\n",
    "        print(self.trough)\n",
    "\n",
    "        for name, conv_block in self.expansive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "        \n",
    "        print('Layer: final\\n', self.final, sep='')\n",
    "\n",
    "# YouNet expects inputs with shape (any_batch_size, any_channel_size, any_even_height, any_even_width)\n",
    "# TODO: Implement padding or shaving of inputs to accept any input height or width (even or odd)\n",
    "# X = torch.randn((32, 3, 320, 480))\n",
    "# net = YouNet(3, 3)\n",
    "\n",
    "# net.print_network()\n",
    "# print()\n",
    "# net.print_forward_hooks()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     preds = net(X)\n",
    "\n",
    "# print()\n",
    "# print('Input shape:', X.shape)\n",
    "# print('Output shape:', preds.shape)\n",
    "# assert preds.shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707f2831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444\n",
      "1426\n",
      "1429\n"
     ]
    }
   ],
   "source": [
    "voc_dir = 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/'\n",
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "crop_size = (256, 256)\n",
    "dataset = 'train'\n",
    "train_set = VocDataset(voc_dir, transform, transform, crop_size, dataset)\n",
    "val_set = VocDataset(voc_dir, transform, transform, crop_size, 'val')\n",
    "test_set = VocDataset(voc_dir, transform, transform, crop_size, 'test')\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70722c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6727, -1.8268, -1.9809,  ...,  0.9303,  1.1187,  1.4098],\n",
       "         [-1.6898, -1.8268, -1.9980,  ...,  0.9474,  1.1700,  1.4612],\n",
       "         [-1.7069, -1.8097, -1.9809,  ...,  0.9646,  1.2214,  1.3927],\n",
       "         ...,\n",
       "         [ 2.0263,  2.1804,  2.2489,  ..., -2.1179, -2.1179, -1.9980],\n",
       "         [ 1.5810,  1.9749,  2.2318,  ..., -2.0494, -2.0665, -2.1179],\n",
       "         [ 0.3823,  0.9474,  1.5810,  ..., -2.0323, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-1.4230, -1.4930, -1.6681,  ...,  1.3606,  1.5532,  1.8333],\n",
       "         [-1.4405, -1.4930, -1.6856,  ...,  1.3782,  1.6057,  1.8508],\n",
       "         [-1.4755, -1.5105, -1.6856,  ...,  1.3957,  1.6583,  1.7808],\n",
       "         ...,\n",
       "         [ 2.0784,  2.3060,  2.4286,  ..., -1.8782, -1.9657, -1.9132],\n",
       "         [ 1.3606,  1.9559,  2.3410,  ..., -1.8431, -1.9657, -2.0357],\n",
       "         [-0.3200,  0.4328,  1.2031,  ..., -1.8782, -1.9832, -2.0357]],\n",
       "\n",
       "        [[-1.5604, -1.6999, -1.7870,  ...,  1.0714,  1.2631,  1.5071],\n",
       "         [-1.5779, -1.6999, -1.8044,  ...,  1.1237,  1.3154,  1.5420],\n",
       "         [-1.6127, -1.6999, -1.8044,  ...,  1.1411,  1.3677,  1.4722],\n",
       "         ...,\n",
       "         [ 1.3328,  1.7685,  2.1868,  ..., -1.8044, -1.7522, -1.6476],\n",
       "         [ 0.3045,  1.0888,  1.7337,  ..., -1.7522, -1.6999, -1.7696],\n",
       "         [-1.3513, -0.6541, -0.0267,  ..., -1.5953, -1.7347, -1.8044]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f8db224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([1, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 1, 256, 256])\n",
      "in_channels=3, out_channels=64\n",
      "in_channels=64, out_channels=128\n",
      "in_channels=128, out_channels=256\n",
      "in_channels=256, out_channels=512\n",
      "in_channels=512, out_channels=1024\n",
      "in_channels=1024, out_channels=512\n",
      "in_channels=512, out_channels=256\n",
      "in_channels=256, out_channels=128\n",
      "in_channels=128, out_channels=64\n",
      "X.shape input=torch.Size([1, 3, 256, 256])\n",
      "X.shape output=torch.Size([1, 64, 256, 256])\n",
      "X.shape input=torch.Size([1, 64, 128, 128])\n",
      "X.shape output=torch.Size([1, 128, 128, 128])\n",
      "X.shape input=torch.Size([1, 128, 64, 64])\n",
      "X.shape output=torch.Size([1, 256, 64, 64])\n",
      "X.shape input=torch.Size([1, 256, 32, 32])\n",
      "X.shape output=torch.Size([1, 512, 32, 32])\n",
      "X.shape input=torch.Size([1, 1024, 16, 16])\n",
      "X.shape output=torch.Size([1, 1024, 32, 32])\n",
      "X.shape input=torch.Size([1, 512, 32, 32])\n",
      "X.shape output=torch.Size([1, 512, 64, 64])\n",
      "X.shape input=torch.Size([1, 256, 64, 64])\n",
      "X.shape output=torch.Size([1, 256, 128, 128])\n",
      "X.shape input=torch.Size([1, 128, 128, 128])\n",
      "X.shape output=torch.Size([1, 128, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "image, mask = train_set[0]\n",
    "print(image.shape, mask.shape)\n",
    "image = torch.unsqueeze(image, dim=0)\n",
    "mask = torch.unsqueeze(mask, dim=0)\n",
    "print(image.shape, mask.shape)\n",
    "mynet = YouNet(3, 3).cuda()\n",
    "with torch.no_grad():\n",
    "    pred = mynet(image.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2269eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "tensor([[[[ 0.0979, -0.8878, -0.1129,  ...,  0.2894, -0.4189,  0.6418],\n",
      "          [-0.6275,  0.1431,  0.4982,  ...,  0.0651,  0.1573,  1.4438],\n",
      "          [ 0.0277, -0.0104, -0.0405,  ..., -0.2748, -0.2521,  0.3780],\n",
      "          ...,\n",
      "          [ 0.2540,  0.6698,  0.6048,  ...,  0.6254,  0.9863,  0.5160],\n",
      "          [ 0.1728,  0.4521,  0.1488,  ...,  0.4075,  0.5467,  0.5657],\n",
      "          [ 0.6099,  0.2704,  0.1651,  ...,  0.2091,  0.4707,  0.2409]],\n",
      "\n",
      "         [[-0.7779, -0.5001, -1.8573,  ..., -1.0764, -1.1688, -0.0066],\n",
      "          [-0.2200,  0.1282, -1.1821,  ..., -0.7356, -1.0485, -0.6675],\n",
      "          [-0.4095, -0.8056, -1.1746,  ...,  0.0949, -1.2966, -0.1276],\n",
      "          ...,\n",
      "          [-0.4032, -0.0628, -0.3616,  ..., -0.5273, -0.6424, -0.5477],\n",
      "          [ 0.0255,  0.1283,  0.1863,  ..., -0.2552, -0.2172, -0.5096],\n",
      "          [ 0.1114, -0.2135, -0.4068,  ..., -0.3544, -1.0439, -0.8867]],\n",
      "\n",
      "         [[ 1.3020, -0.2616,  0.2513,  ...,  0.2454,  0.5113, -0.1288],\n",
      "          [-0.2542, -0.7952, -0.4766,  ..., -0.5847,  0.9514, -0.5164],\n",
      "          [ 0.5032, -0.2350, -0.4644,  ..., -0.9667,  0.3897, -0.5950],\n",
      "          ...,\n",
      "          [-0.6134, -0.6049, -0.1198,  ..., -0.2514,  0.9775, -0.0082],\n",
      "          [-0.6042, -0.3650, -0.0513,  ...,  0.0989,  0.5750,  0.1029],\n",
      "          [-0.3510, -0.5467, -0.2407,  ..., -0.1104,  0.0219,  0.1418]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36105c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
