{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8751583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a60ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "        \n",
    "        nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1),\n",
    "        nn.BatchNorm2d(num_features=out_channels),\n",
    "        nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb251dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    skip_connections = []\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encode=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.encode = encode\n",
    "        self.conv = build_conv_block(in_channels=self.in_channels, out_channels=self.out_channels)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.encode:\n",
    "            X = self.conv(X)\n",
    "            self.skip_connections.append(X)\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)(X)\n",
    "        else:\n",
    "            X = nn.ConvTranspose2d(in_channels=self.in_channels, out_channels=self.in_channels//2, kernel_size=2, stride=2)(X)\n",
    "            X = torch.cat((X, self.skip_connections.pop()), dim=1)\n",
    "            return self.conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "023ff8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encode0\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: encode1\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: encode2\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: encode3\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: bottleneck\n",
      "Sequential(\n",
      "  (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      ")\n",
      "Layer: decode3\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: decode2\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: decode1\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: decode0\n",
      "ConvBlock(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Layer: final\n",
      "Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "\n",
      "torch.Size([32, 8, 160, 160])\n",
      "torch.Size([32, 8, 160, 160])\n"
     ]
    }
   ],
   "source": [
    "class YouNet(nn.Module):\n",
    "    def __init__(self, in_channels=64, out_channels=64):\n",
    "        super(YouNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # The down-sampling layers\n",
    "        self.contractive_path = nn.ModuleDict({\n",
    "            'encode0': ConvBlock(in_channels=self.in_channels, out_channels=64, encode=True),\n",
    "            'encode1': ConvBlock(in_channels=64, out_channels=128, encode=True),\n",
    "            'encode2': ConvBlock(in_channels=128, out_channels=256, encode=True),\n",
    "            'encode3': ConvBlock(in_channels=256, out_channels=512, encode=True),\n",
    "        })\n",
    "\n",
    "        # The bottleneck\n",
    "        self.trough = build_conv_block(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # The up-sampling layers\n",
    "        # in_channels takes input from previous layer and skip connections\n",
    "        self.expansive_path = nn.ModuleDict({\n",
    "            'decode3': ConvBlock(in_channels=512*2, out_channels=512, encode=False),\n",
    "            'decode2': ConvBlock(in_channels=256*2, out_channels=256, encode=False),\n",
    "            'decode1': ConvBlock(in_channels=128*2, out_channels=128, encode=False),\n",
    "            'decode0': ConvBlock(in_channels=64*2, out_channels=64, encode=False)\n",
    "        })\n",
    "\n",
    "        # The prediction layer\n",
    "        self.final = nn.Conv2d(in_channels=64, out_channels=self.out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Train the contractive path\n",
    "        for conv_block in self.contractive_path:\n",
    "            X = self.contractive_path[conv_block](X)\n",
    "        \n",
    "        # Train the trough\n",
    "        X = self.trough(X)\n",
    "        \n",
    "        # Train the expansive path\n",
    "        for conv_block in self.expansive_path:\n",
    "            X = self.expansive_path[conv_block](X)\n",
    "        \n",
    "        return self.final(X)\n",
    "    \n",
    "    def print_hook_shape(self, module, input, output):\n",
    "        '''Prints the input and output tensor of a given layer. Used by YouNet.print_forward_hooks'''\n",
    "        print(f'{module.__class__.__name__}(input shape: {input[0].shape}, output shape: {output.shape})')\n",
    "\n",
    "    def print_forward_hooks(self):\n",
    "        '''Prints the input and output tensors of each layer.'''\n",
    "        for name, layer in self.contractive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.trough.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        for name, layer in self.expansive_path.items():\n",
    "            layer.register_forward_hook(self.print_hook_shape)\n",
    "        \n",
    "        self.final.register_forward_hook(self.print_hook_shape)\n",
    "    \n",
    "    def print_network(self):\n",
    "        '''Prints the entire network architecture.'''\n",
    "        for name, conv_block in self.contractive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "\n",
    "        print('Layer: bottleneck')\n",
    "        print(self.trough)\n",
    "\n",
    "        for name, conv_block in self.expansive_path.items():\n",
    "            print('Layer:', name)\n",
    "            print(conv_block)\n",
    "        \n",
    "        print('Layer: final\\n', self.final, sep='')\n",
    "\n",
    "net = YouNet(8, 8)\n",
    "net.print_network()\n",
    "X = torch.randn((32, 8, 160, 160))\n",
    "preds = net(X)\n",
    "print()\n",
    "print(X.shape)\n",
    "print(preds.shape)\n",
    "assert preds.shape == X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
