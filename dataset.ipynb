{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56018fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import models, transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "from patchify import patchify, unpatchify\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9dab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir = 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_dimensions(folder_path):\n",
    "    min_width = float('inf')\n",
    "    max_width = float('-inf')\n",
    "    min_height = float('inf')\n",
    "    max_height = float('-inf')\n",
    "    total_width = 0\n",
    "    total_height = 0\n",
    "    total_images = 0\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))]\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No image files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "    for file_name in image_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                width, height = img.size\n",
    "                min_width = min(min_width, width)\n",
    "                max_width = max(max_width, width)\n",
    "                min_height = min(min_height, height)\n",
    "                max_height = max(max_height, height)\n",
    "                total_width = total_width + width\n",
    "                total_height = total_height + height\n",
    "                total_images = total_images + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    return {\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'total_width': total_width,\n",
    "        'total_height': total_height,\n",
    "        'total_images': total_images\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = get_image_dimensions(os.path.join(voc_dir, 'SegmentationClass'))\n",
    "if result:\n",
    "    print(\"Image Dimension Summary:\")\n",
    "    print(f\"Min Width: {result['min_width']}\")\n",
    "    print(f\"Max Width: {result['max_width']}\")\n",
    "    print(f\"Min Height: {result['min_height']}\")\n",
    "    print(f\"Max Height: {result['max_height']}\")\n",
    "    print(f\"Average Width: {result['total_width'] / result['total_images']}\")\n",
    "    print(f\"Average Height: {result['total_height'] / result['total_images']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(data_dir, is_train):\n",
    "    '''Reads and returns all images and their respective masks.'''\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if is_train else 'val.txt')\n",
    "    rgb_mode = torchvision.io.image.ImageReadMode.RGB\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'), rgb_mode)\n",
    "        )\n",
    "    \n",
    "    return images, masks, image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e032ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(patch_size, img_dir):\n",
    "    '''Converts an image into several patches.'''\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "\n",
    "    # Compute padding needed to make dimensions divisible by patch_size\n",
    "    pad_h = (patch_size - img.height % patch_size) % patch_size\n",
    "    pad_w = (patch_size - img.width  % patch_size) % patch_size\n",
    "    pad = T.Pad((0, 0, pad_w, pad_h))  # Pad right and bottom\n",
    "    img_padded = pad(img)\n",
    "\n",
    "    img_array = np.array(img_padded)\n",
    "    patches = patchify(img_array, (patch_size, patch_size, 3), step=patch_size)\n",
    "\n",
    "    # print(patches.shape)\n",
    "    return patches\n",
    "\n",
    "def read_and_patch_images(data_dir, is_train):\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if is_train else 'val.txt')\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'))\n",
    "        )\n",
    "    # print(len(images[0]), len(images[0][1]), len(images[0][1][2]), len(images[3]))\n",
    "    images, masks = np.concatenate(images), np.concatenate(masks)\n",
    "    # print(images.shape, masks.shape)\n",
    "    # print('rapi:', images[0].shape)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe226c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_patches(patches):\n",
    "    '''Displays all patches in an image in their respective positions in the image.'''\n",
    "    n_rows, n_cols = patches.shape[0], patches.shape[1]\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2, n_rows*2))\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            patch = patches[i, j, 0]  # shape: (patch_size, patch_size, 3)\n",
    "            axes[i, j].imshow(patch)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/',\n",
    "    read_images(voc_dir, False)[2][10] + '.jpg'\n",
    ")\n",
    "display_patches(patch_image(128, image))\n",
    "mask = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/',\n",
    "    read_images(voc_dir, False)[2][10] + '.png'\n",
    ")\n",
    "display_patches(patch_image(128, mask))\n",
    "\n",
    "patches1 = patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/2007_000033.png')\n",
    "display_patches(patches1)\n",
    "display_patches(patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/2007_000033.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR torch.Size([17900, 3, 128, 128]) <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class VocDataset(Dataset):\n",
    "    '''Dataset that pre-loads the PASCAL2 VOC 2012 dataset into RAM. This implies that the DataLoader must have num_workers=0. The argument \"transforms\" must be a transformation ending with ToTensor.'''\n",
    "    def __init__(self, data_dir, transforms, is_train):\n",
    "        self.data_dir = data_dir\n",
    "        self.filter = filter\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.images, self.masks = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images, masks = read_and_patch_images(self.data_dir, self.is_train)\n",
    "        images = images.squeeze(1)\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "        images = torch.stack([\n",
    "            self.normalize(self.transforms(image).to(torch.float32))\n",
    "            for image in images\n",
    "        ])\n",
    "        masks = torch.stack([\n",
    "            self.transforms(mask).to(torch.float32)\n",
    "            for mask in masks\n",
    "        ])\n",
    "        print('TENSOR', images.shape, type(images), type(images[0]), type(images[0][0]), type(images[0][0][0]))\n",
    "        \n",
    "        return images, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.masks[index]\n",
    "\n",
    "transforms = torchvision.transforms.ToTensor()\n",
    "train_set = VocDataset(voc_dir, transforms, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce897527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 69\n",
      "X shape: torch.Size([256, 3, 128, 128])\n",
      "Y shape: torch.Size([256, 3, 128, 128])\n",
      "tensor([[[[ 0.9646,  0.7248,  0.8961,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.9132,  1.1358,  1.2728,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 1.3755,  1.4440,  1.3584,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-1.3987, -1.7412, -1.7583,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-1.2445, -1.6384, -1.7412,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-0.6965, -1.1932, -1.0219,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[ 1.0630,  0.7479,  0.7479,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.9755,  1.1331,  1.1506,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 1.4132,  1.3957,  1.2381,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-1.0378, -1.3880, -1.4230,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-0.9853, -1.4405, -1.4930,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-0.5126, -1.0553, -0.8452,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[ 0.9494,  0.6531,  0.7925,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.8622,  1.0539,  1.1411,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 1.3502,  1.3677,  1.1759,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.3513, -1.6999, -1.6302,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.2641, -1.6999, -1.6302,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-0.7238, -1.2467, -0.9156,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1008, -2.0837, -2.1008,  ...,  0.3823,  0.5364,  0.5022],\n",
      "          [-2.1179, -2.0837, -2.1008,  ...,  0.2796,  0.3994,  0.4337],\n",
      "          [-2.0837, -2.1008, -2.1179,  ...,  0.2796,  0.3309,  0.4166],\n",
      "          ...,\n",
      "          [-0.9534,  0.1426, -0.4568,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-0.5767,  0.3481, -1.4158,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.0741, -0.0458, -1.8268,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-1.9657, -1.9482, -1.9482,  ...,  1.4307,  1.5357,  1.4657],\n",
      "          [-1.9832, -1.9832, -2.0007,  ...,  1.4307,  1.5007,  1.5182],\n",
      "          [-1.9832, -2.0007, -2.0357,  ...,  1.4482,  1.4657,  1.5532],\n",
      "          ...,\n",
      "          [ 0.4853,  1.1331,  0.3102,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.7129,  1.3606, -0.8978,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 1.0455,  0.8529, -1.6155,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.6476, -1.6302, -1.6650,  ...,  1.8383,  1.8731,  1.7860],\n",
      "          [-1.6650, -1.6824, -1.6999,  ...,  1.7163,  1.7685,  1.7685],\n",
      "          [-1.6824, -1.6999, -1.7696,  ...,  1.8034,  1.8383,  1.8905],\n",
      "          ...,\n",
      "          [ 1.7511,  1.9254,  1.0017,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 1.7163,  2.0474, -0.4624,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 1.9603,  1.5071, -1.1073,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4679,  0.4508,  0.4337,  ...,  0.7762,  0.7419,  0.7762],\n",
      "          [ 0.5193,  0.5022,  0.4851,  ...,  0.8104,  0.7933,  0.8104],\n",
      "          [ 0.5193,  0.5022,  0.4851,  ...,  0.8104,  0.7933,  0.8104],\n",
      "          ...,\n",
      "          [ 0.7248,  0.7248,  0.7248,  ...,  0.2624,  0.3652,  0.4337],\n",
      "          [ 0.7419,  0.7419,  0.7248,  ...,  0.2111,  0.2967,  0.3994],\n",
      "          [ 0.6734,  0.6906,  0.6906,  ...,  0.2967,  0.3309,  0.3481]],\n",
      "\n",
      "         [[ 0.5203,  0.5028,  0.4853,  ...,  0.8354,  0.8004,  0.8529],\n",
      "          [ 0.5728,  0.5553,  0.5378,  ...,  0.8704,  0.8529,  0.8704],\n",
      "          [ 0.6078,  0.5903,  0.5728,  ...,  0.9055,  0.8880,  0.9055],\n",
      "          ...,\n",
      "          [ 0.8529,  0.8354,  0.8354,  ...,  0.1001,  0.2052,  0.2752],\n",
      "          [ 0.8704,  0.8529,  0.8354,  ...,  0.0126,  0.1001,  0.2052],\n",
      "          [ 0.7654,  0.7654,  0.8004,  ...,  0.1001,  0.1352,  0.1527]],\n",
      "\n",
      "         [[ 0.4091,  0.3916,  0.3742,  ...,  0.7054,  0.6356,  0.6356],\n",
      "          [ 0.4614,  0.4439,  0.4265,  ...,  0.7402,  0.7228,  0.7402],\n",
      "          [ 0.4962,  0.4788,  0.4614,  ...,  0.7925,  0.7751,  0.7925],\n",
      "          ...,\n",
      "          [ 0.7576,  0.8274,  0.8274,  ...,  0.0431,  0.1476,  0.2173],\n",
      "          [ 0.7751,  0.8448,  0.8448,  ..., -0.0267,  0.0605,  0.1651],\n",
      "          [ 0.6879,  0.7751,  0.8099,  ...,  0.0605,  0.0953,  0.1128]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4568, -0.4397, -0.6623,  ..., -0.5082,  0.3994, -1.0904],\n",
      "          [ 0.0227, -0.1657, -0.3712,  ..., -0.4054, -0.4911, -1.1932],\n",
      "          [-0.7479,  0.2282, -1.7754,  ...,  0.0227, -0.2684, -0.1314],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-0.4601, -0.4076, -0.6176,  ..., -0.6527,  0.1702, -1.3529],\n",
      "          [ 0.0651, -0.1275, -0.3200,  ..., -0.5126, -0.7227, -1.4405],\n",
      "          [-0.6877,  0.3102, -1.7031,  ..., -0.1099, -0.4951, -0.3901],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-0.3404, -0.2358, -0.4450,  ..., -0.9330, -0.1138, -1.5953],\n",
      "          [ 0.1825,  0.0431, -0.1487,  ..., -0.8458, -1.1073, -1.8044],\n",
      "          [-0.6193,  0.4439, -1.5604,  ..., -0.4624, -0.8807, -0.8458],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9817,  0.7933,  0.7762,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.8447,  0.7933,  0.8961,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.6734,  0.7762,  0.7933,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [ 0.5707,  0.8789,  1.3070,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.9988,  1.0502,  1.0331,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 1.5468,  0.8618,  0.9132,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[ 0.9405,  0.7829,  0.7654,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.8004,  0.7829,  0.8880,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.6078,  0.7129,  0.7304,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [ 0.6604,  0.9230,  1.2206,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 1.0630,  1.0630,  0.9055,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 1.6232,  0.8880,  0.7829,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[ 0.8448,  0.7054,  0.6879,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.7054,  0.7054,  0.8099,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.5485,  0.6531,  0.7054,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [ 0.2696,  0.5136,  0.9842,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.8971,  0.8797,  0.8448,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 1.5071,  0.7925,  0.7751,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2489,  2.1119,  2.1462,  ...,  1.6667,  1.4098,  1.0502],\n",
      "          [ 2.1975,  2.2489,  1.9578,  ...,  2.1975,  1.1529,  1.1872],\n",
      "          [ 2.2489,  2.1975,  1.6324,  ...,  1.3927,  1.3927,  0.8961],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-0.8102, -0.6352, -0.1975,  ...,  1.5707,  1.3606,  1.1681],\n",
      "          [-0.8102, -0.8102, -0.4776,  ...,  2.0609,  1.1856,  1.2556],\n",
      "          [-0.8803, -0.6001, -0.9153,  ...,  1.4307,  1.3957,  1.0455],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-0.3230, -0.5147,  0.1999,  ...,  1.2108,  0.7402,  0.3219],\n",
      "          [-0.3753, -0.4798, -0.1487,  ...,  1.8557,  0.6705,  0.5136],\n",
      "          [-0.3578, -0.3230, -0.6193,  ...,  1.1934,  0.8971,  0.2173],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n"
     ]
    }
   ],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, 256, shuffle=True, drop_last=True, num_workers=0)\n",
    "print('Total number of batches:', len(train_iter))\n",
    "for X, Y in train_iter:\n",
    "    print('X shape:', X.shape)\n",
    "    print('Y shape:', Y.shape)\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42e2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
