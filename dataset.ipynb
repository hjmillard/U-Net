{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56018fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import models, transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "from patchify import patchify, unpatchify\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9dab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir = 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_dimensions(folder_path):\n",
    "    min_width = float('inf')\n",
    "    max_width = float('-inf')\n",
    "    min_height = float('inf')\n",
    "    max_height = float('-inf')\n",
    "    total_width = 0\n",
    "    total_height = 0\n",
    "    total_images = 0\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))]\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No image files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "    for file_name in image_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                width, height = img.size\n",
    "                min_width = min(min_width, width)\n",
    "                max_width = max(max_width, width)\n",
    "                min_height = min(min_height, height)\n",
    "                max_height = max(max_height, height)\n",
    "                total_width = total_width + width\n",
    "                total_height = total_height + height\n",
    "                total_images = total_images + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    return {\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'total_width': total_width,\n",
    "        'total_height': total_height,\n",
    "        'total_images': total_images\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = get_image_dimensions(os.path.join(voc_dir, 'SegmentationClass'))\n",
    "if result:\n",
    "    print(\"Image Dimension Summary:\")\n",
    "    print(f\"Min Width: {result['min_width']}\")\n",
    "    print(f\"Max Width: {result['max_width']}\")\n",
    "    print(f\"Min Height: {result['min_height']}\")\n",
    "    print(f\"Max Height: {result['max_height']}\")\n",
    "    print(f\"Average Width: {result['total_width'] / result['total_images']}\")\n",
    "    print(f\"Average Height: {result['total_height'] / result['total_images']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ee915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(data_dir, dataset_type):\n",
    "    '''Reads and returns all images and their respective masks.'''\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if dataset_type else 'val.txt')\n",
    "    rgb_mode = torchvision.io.image.ImageReadMode.RGB\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'), rgb_mode)\n",
    "        )\n",
    "    \n",
    "    return images, masks, image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e032ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(patch_size, img_dir):\n",
    "    '''Converts an image into several patches.'''\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "\n",
    "    # Compute padding needed to make dimensions divisible by patch_size\n",
    "    pad_h = (patch_size - img.height % patch_size) % patch_size\n",
    "    pad_w = (patch_size - img.width  % patch_size) % patch_size\n",
    "    pad = T.Pad((0, 0, pad_w, pad_h))  # Pad right and bottom\n",
    "    img_padded = pad(img)\n",
    "\n",
    "    img_array = np.array(img_padded)\n",
    "    patches = patchify(img_array, (patch_size, patch_size, 3), step=patch_size)\n",
    "\n",
    "    # print(patches.shape)\n",
    "    return patches\n",
    "\n",
    "def read_and_patch_images(data_dir, dataset_type):\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', dataset_type+'.txt')\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'))\n",
    "        )\n",
    "    # print(len(images[0]), len(images[0][1]), len(images[0][1][2]), len(images[3]))\n",
    "    images, masks = np.concatenate(images), np.concatenate(masks)\n",
    "    # print(images.shape, masks.shape)\n",
    "    # print('rapi:', images[0].shape)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe226c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_patches(patches):\n",
    "    '''Displays all patches in an image in their respective positions in the image.'''\n",
    "    n_rows, n_cols = patches.shape[0], patches.shape[1]\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2, n_rows*2))\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            patch = patches[i, j, 0]  # shape: (patch_size, patch_size, 3)\n",
    "            axes[i, j].imshow(patch)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/',\n",
    "    read_images(voc_dir, False)[2][10] + '.jpg'\n",
    ")\n",
    "display_patches(patch_image(128, image))\n",
    "mask = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/',\n",
    "    read_images(voc_dir, False)[2][10] + '.png'\n",
    ")\n",
    "display_patches(patch_image(128, mask))\n",
    "\n",
    "patches1 = patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/2007_000033.png')\n",
    "display_patches(patches1)\n",
    "display_patches(patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/2007_000033.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e8387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR torch.Size([17900, 3, 128, 128]) <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class VocDataset(Dataset):\n",
    "    '''Dataset that pre-loads the PASCAL2 VOC 2012 dataset into RAM. This implies that the DataLoader must have num_workers=0. The argument \"transforms\" must be a transformation ending with ToTensor.'''\n",
    "    def __init__(self, data_dir, transforms, dataset_type):\n",
    "        self.data_dir = data_dir\n",
    "        self.filter = filter\n",
    "        self.transforms = transforms\n",
    "        self.dataset_type = dataset_type\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.images, self.masks = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images, masks = read_and_patch_images(self.data_dir, self.dataset_type)\n",
    "        images = images.squeeze(1)\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "        images = torch.stack([\n",
    "            self.normalize(self.transforms(image).to(torch.float32))\n",
    "            for image in images\n",
    "        ])\n",
    "        masks = torch.stack([\n",
    "            self.transforms(mask).to(torch.float32)\n",
    "            for mask in masks\n",
    "        ])\n",
    "        print('TENSOR', images.shape, type(images), type(images[0]), type(images[0][0]), type(images[0][0][0]))\n",
    "        \n",
    "        return images, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.masks[index]\n",
    "\n",
    "transforms = torchvision.transforms.ToTensor()\n",
    "train_set = VocDataset(voc_dir, transforms, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce897527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches: 69\n",
      "X shape: torch.Size([256, 3, 128, 128])\n",
      "Y shape: torch.Size([256, 3, 128, 128])\n",
      "tensor([[[[-0.2342, -0.1999, -0.1314,  ...,  0.1083,  0.1597,  0.0912],\n",
      "          [-0.1314, -0.1486, -0.1486,  ...,  0.1939,  0.1768,  0.0227],\n",
      "          [-0.1143, -0.1486, -0.1657,  ...,  0.1426,  0.2111,  0.1768],\n",
      "          ...,\n",
      "          [ 0.1768,  0.1768,  0.2111,  ...,  0.2796,  0.2453,  0.0569],\n",
      "          [ 0.1254,  0.1254,  0.1768,  ...,  0.5022,  0.7762,  0.3652],\n",
      "          [ 0.1597,  0.1597,  0.2111,  ...,  0.4679,  0.7933,  0.4679]],\n",
      "\n",
      "         [[-0.4251, -0.3901, -0.3725,  ...,  0.0826,  0.2227,  0.1001],\n",
      "          [-0.3725, -0.3901, -0.3901,  ...,  0.1176,  0.2052,  0.0301],\n",
      "          [-0.3901, -0.4251, -0.4426,  ...,  0.0651,  0.2402,  0.1527],\n",
      "          ...,\n",
      "          [-0.1450, -0.1450, -0.1099,  ..., -0.1975, -0.1450, -0.2150],\n",
      "          [-0.1975, -0.1975, -0.1450,  ...,  0.0301,  0.3978,  0.1527],\n",
      "          [-0.1625, -0.1625, -0.1099,  ..., -0.0049,  0.4328,  0.2752]],\n",
      "\n",
      "         [[-0.5844, -0.5495, -0.5147,  ...,  0.0082,  0.2348,  0.1825],\n",
      "          [-0.4450, -0.4624, -0.4624,  ...,  0.0605,  0.2173,  0.1128],\n",
      "          [-0.4798, -0.5147, -0.5321,  ...,  0.0082,  0.2522,  0.2173],\n",
      "          ...,\n",
      "          [-0.3927, -0.3927, -0.3578,  ..., -0.5321, -0.5147, -0.5844],\n",
      "          [-0.4450, -0.4450, -0.3927,  ..., -0.3055,  0.0256, -0.2358],\n",
      "          [-0.3753, -0.4101, -0.3578,  ..., -0.3055,  0.0953, -0.0790]]],\n",
      "\n",
      "\n",
      "        [[[-0.1657, -0.0458, -0.0801,  ..., -1.7240, -1.8097, -1.8782],\n",
      "          [-0.2171, -0.0801, -0.2513,  ..., -1.8610, -1.8782, -1.8610],\n",
      "          [-0.2513, -0.0972, -0.1314,  ..., -1.9295, -1.8782, -1.8610],\n",
      "          ...,\n",
      "          [-1.7583, -1.7925, -1.7754,  ..., -1.7240, -1.7412, -1.7412],\n",
      "          [-1.7412, -1.7754, -1.6898,  ..., -1.7412, -1.7583, -1.7240],\n",
      "          [-1.6384, -1.7925, -1.6384,  ..., -1.8097, -1.7754, -1.7240]],\n",
      "\n",
      "         [[-0.0574,  0.0651,  0.2227,  ..., -1.7206, -1.7031, -1.6856],\n",
      "          [-0.0924,  0.0301,  0.0126,  ..., -1.7731, -1.7906, -1.7906],\n",
      "          [-0.1275,  0.0301,  0.1352,  ..., -1.8081, -1.7556, -1.7031],\n",
      "          ...,\n",
      "          [-1.4755, -1.4755, -1.4580,  ..., -1.5280, -1.5455, -1.5455],\n",
      "          [-1.5105, -1.4930, -1.4055,  ..., -1.5455, -1.5630, -1.5280],\n",
      "          [-1.4930, -1.5630, -1.4055,  ..., -1.6155, -1.5805, -1.5280]],\n",
      "\n",
      "         [[ 0.2522,  0.3742,  0.4788,  ..., -1.5430, -1.5604, -1.5256],\n",
      "          [ 0.1651,  0.3393,  0.2696,  ..., -1.5779, -1.5953, -1.5953],\n",
      "          [ 0.1302,  0.2871,  0.3916,  ..., -1.6302, -1.5430, -1.4907],\n",
      "          ...,\n",
      "          [-1.5430, -1.5604, -1.5430,  ..., -1.5430, -1.5604, -1.5604],\n",
      "          [-1.5430, -1.5430, -1.4559,  ..., -1.5604, -1.5779, -1.5430],\n",
      "          [-1.4559, -1.5604, -1.4036,  ..., -1.6302, -1.5604, -1.5081]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2385,  1.2557,  1.1700,  ...,  0.7248,  0.7762,  0.6563],\n",
      "          [ 1.1358,  1.1187,  1.1529,  ...,  0.6392,  0.6221,  0.7248],\n",
      "          [ 1.0673,  1.1015,  1.1015,  ...,  0.4679,  0.3994,  0.4851],\n",
      "          ...,\n",
      "          [ 1.0159,  1.0844,  1.1529,  ...,  0.8276,  0.4679,  1.0844],\n",
      "          [ 0.8789,  1.0502,  1.0159,  ...,  0.5364,  0.0569,  0.6049],\n",
      "          [ 0.4679,  0.6563,  0.9132,  ...,  0.1426,  0.2796,  0.4337]],\n",
      "\n",
      "         [[ 1.5007,  1.5182,  1.4307,  ...,  0.9580,  1.0105,  0.8880],\n",
      "          [ 1.3957,  1.3782,  1.4132,  ...,  0.8704,  0.8529,  0.9580],\n",
      "          [ 1.3256,  1.3606,  1.3606,  ...,  0.6954,  0.6254,  0.7129],\n",
      "          ...,\n",
      "          [ 1.2906,  1.3606,  1.3957,  ...,  1.1856,  0.8354,  1.4657],\n",
      "          [ 1.3606,  1.4482,  1.3431,  ...,  0.8529,  0.3978,  0.9580],\n",
      "          [ 1.0280,  1.1331,  1.2731,  ...,  0.4503,  0.5903,  0.7654]],\n",
      "\n",
      "         [[ 1.6814,  1.6988,  1.6117,  ...,  1.2282,  1.2805,  1.1585],\n",
      "          [ 1.5768,  1.5594,  1.5942,  ...,  1.1411,  1.1237,  1.2282],\n",
      "          [ 1.5071,  1.5420,  1.5420,  ...,  0.9668,  0.8971,  0.9842],\n",
      "          ...,\n",
      "          [ 0.3219,  0.3916,  0.4439,  ...,  0.2522, -0.1487,  0.4788],\n",
      "          [ 0.1999,  0.3568,  0.2871,  ..., -0.0267, -0.4973,  0.0605],\n",
      "          [-0.2532, -0.0615,  0.1651,  ..., -0.3927, -0.2358, -0.0615]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.3755,  1.4098,  1.3927,  ...,  1.5297,  1.5468,  1.5468],\n",
      "          [ 1.1872,  1.2214,  1.2214,  ...,  1.5639,  1.5639,  1.5639],\n",
      "          [ 1.0673,  1.0844,  1.1187,  ...,  1.5810,  1.5810,  1.5982],\n",
      "          ...,\n",
      "          [ 0.2111,  0.1426,  0.0912,  ...,  0.8276,  0.8447,  0.8447],\n",
      "          [ 0.2624,  0.1768,  0.1768,  ...,  0.8447,  0.8618,  0.8789],\n",
      "          [ 0.2453,  0.2282,  0.2453,  ...,  0.8789,  0.8961,  0.8961]],\n",
      "\n",
      "         [[ 0.9755,  1.0105,  1.0455,  ...,  1.3081,  1.3256,  1.3256],\n",
      "          [ 0.9405,  0.9230,  0.9230,  ...,  1.3431,  1.3431,  1.3431],\n",
      "          [ 0.8880,  0.9055,  0.9055,  ...,  1.3606,  1.3606,  1.3256],\n",
      "          ...,\n",
      "          [ 0.1702,  0.0826,  0.0301,  ...,  0.6429,  0.6604,  0.6604],\n",
      "          [ 0.2227,  0.1352,  0.1176,  ...,  0.6604,  0.6779,  0.6954],\n",
      "          [ 0.2927,  0.2227,  0.2227,  ...,  0.6954,  0.7129,  0.7129]],\n",
      "\n",
      "         [[ 0.9668,  0.9668,  0.9842,  ...,  1.1585,  1.1759,  1.1759],\n",
      "          [ 0.8797,  0.8797,  0.8448,  ...,  1.1934,  1.1934,  1.1585],\n",
      "          [ 0.8448,  0.8622,  0.8274,  ...,  1.1759,  1.1759,  1.1585],\n",
      "          ...,\n",
      "          [ 0.3568,  0.2696,  0.2173,  ...,  0.6008,  0.6182,  0.6182],\n",
      "          [ 0.4091,  0.3219,  0.2696,  ...,  0.6182,  0.6356,  0.6531],\n",
      "          [ 0.4614,  0.3916,  0.3568,  ...,  0.6531,  0.6705,  0.6705]]],\n",
      "\n",
      "\n",
      "        [[[-0.8678, -0.8164, -1.0219,  ..., -1.0219, -1.0733, -1.0733],\n",
      "          [-0.8507, -1.0219, -0.9192,  ..., -0.2342, -0.2342, -0.2856],\n",
      "          [-0.7650, -0.6794, -0.7822,  ..., -0.5767, -0.5253, -0.6109],\n",
      "          ...,\n",
      "          [ 0.2967,  0.3138,  0.3481,  ...,  0.5536,  0.7591,  0.8447],\n",
      "          [ 0.3994,  0.4166,  0.3994,  ...,  0.4851,  0.7419,  0.5022],\n",
      "          [ 0.4508,  0.3309,  0.2282,  ...,  0.5022,  0.7762,  0.3138]],\n",
      "\n",
      "         [[-1.0553, -0.9503, -1.1604,  ..., -0.7577, -0.7927, -0.7927],\n",
      "          [-1.0203, -1.2129, -1.1078,  ...,  0.0476,  0.0651,  0.0651],\n",
      "          [-0.9503, -0.9153, -1.1078,  ..., -0.2675, -0.2150, -0.2675],\n",
      "          ...,\n",
      "          [-0.7227, -0.7052, -0.7052,  ...,  0.6429,  0.8880,  0.9930],\n",
      "          [-0.7052, -0.6877, -0.7052,  ...,  0.5728,  0.8529,  0.6078],\n",
      "          [-0.4076, -0.6001, -0.7577,  ...,  0.5553,  0.8354,  0.3803]],\n",
      "\n",
      "         [[-1.1073, -0.8807, -0.9853,  ..., -0.3753, -0.4101, -0.3927],\n",
      "          [-1.0201, -1.1247, -1.0027,  ...,  0.4265,  0.4439,  0.4439],\n",
      "          [-0.8458, -0.8110, -1.0027,  ...,  0.0605,  0.1128,  0.0953],\n",
      "          ...,\n",
      "          [-0.2532, -0.2358, -0.2184,  ...,  1.0539,  1.2108,  1.1759],\n",
      "          [-0.1661, -0.1487, -0.1661,  ...,  0.9145,  1.1585,  0.8448],\n",
      "          [ 0.0779, -0.0964, -0.2358,  ...,  0.8448,  1.1237,  0.6182]]],\n",
      "\n",
      "\n",
      "        [[[-0.7650, -0.7308, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-1.1247, -0.7479, -0.5596,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-1.1589, -2.1008, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-0.8803, -0.6352, -2.0007,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.2479, -0.8452, -0.5651,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.1604, -2.0182, -1.8957,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-0.9504, -0.7238, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.3164, -1.0027, -0.8458,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.1421, -1.7870, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n"
     ]
    }
   ],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, 256, shuffle=True, drop_last=True, num_workers=0)\n",
    "print('Total number of batches:', len(train_iter))\n",
    "for X, Y in train_iter:\n",
    "    print('X shape:', X.shape)\n",
    "    print('Y shape:', Y.shape)\n",
    "    print(X)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
