{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56018fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import models, transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "from patchify import patchify, unpatchify\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb9dab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir = 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_dimensions(folder_path):\n",
    "    min_width = float('inf')\n",
    "    max_width = float('-inf')\n",
    "    min_height = float('inf')\n",
    "    max_height = float('-inf')\n",
    "    total_width = 0\n",
    "    total_height = 0\n",
    "    total_images = 0\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))]\n",
    "\n",
    "    if not image_files:\n",
    "        print(\"No image files found in the folder.\")\n",
    "        return None\n",
    "\n",
    "    for file_name in image_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                width, height = img.size\n",
    "                min_width = min(min_width, width)\n",
    "                max_width = max(max_width, width)\n",
    "                min_height = min(min_height, height)\n",
    "                max_height = max(max_height, height)\n",
    "                total_width = total_width + width\n",
    "                total_height = total_height + height\n",
    "                total_images = total_images + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    return {\n",
    "        'min_width': min_width,\n",
    "        'max_width': max_width,\n",
    "        'min_height': min_height,\n",
    "        'max_height': max_height,\n",
    "        'total_width': total_width,\n",
    "        'total_height': total_height,\n",
    "        'total_images': total_images\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = get_image_dimensions(os.path.join(voc_dir, 'SegmentationClass'))\n",
    "if result:\n",
    "    print(\"Image Dimension Summary:\")\n",
    "    print(f\"Min Width: {result['min_width']}\")\n",
    "    print(f\"Max Width: {result['max_width']}\")\n",
    "    print(f\"Min Height: {result['min_height']}\")\n",
    "    print(f\"Max Height: {result['max_height']}\")\n",
    "    print(f\"Average Width: {result['total_width'] / result['total_images']}\")\n",
    "    print(f\"Average Height: {result['total_height'] / result['total_images']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(data_dir, is_train):\n",
    "    '''Reads and returns all images and their respective masks.'''\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if is_train else 'val.txt')\n",
    "    rgb_mode = torchvision.io.image.ImageReadMode.RGB\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.append(\n",
    "            torchvision.io.read_image(os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'), rgb_mode)\n",
    "        )\n",
    "    \n",
    "    return images, masks, image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e032ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_image(patch_size, img_dir):\n",
    "    '''Converts an image into several patches.'''\n",
    "    img = Image.open(img_dir).convert('RGB')\n",
    "\n",
    "    # Compute padding needed to make dimensions divisible by patch_size\n",
    "    pad_h = (patch_size - img.height % patch_size) % patch_size\n",
    "    pad_w = (patch_size - img.width  % patch_size) % patch_size\n",
    "    pad = T.Pad((0, 0, pad_w, pad_h))  # Pad right and bottom\n",
    "    img_padded = pad(img)\n",
    "\n",
    "    img_array = np.array(img_padded)\n",
    "    patches = patchify(img_array, (patch_size, patch_size, 3), step=patch_size)\n",
    "\n",
    "    return patches\n",
    "\n",
    "def read_and_patch_images(data_dir, is_train):\n",
    "    annotations_file = os.path.join(data_dir, 'ImageSets', 'Segmentation', 'train.txt' if is_train else 'val.txt')\n",
    "\n",
    "    with open(annotations_file, 'r') as f:\n",
    "        image_names = f.read().split()\n",
    "    \n",
    "    images, masks = [], []\n",
    "    for file_name in image_names:\n",
    "        images.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'JPEGImages', f'{file_name}.jpg'))\n",
    "        )\n",
    "        masks.extend(\n",
    "            patch_image(128, os.path.join(data_dir, 'SegmentationClass', f'{file_name}.png'))\n",
    "        )\n",
    "    \n",
    "    return torch.tensor(images), torch.tensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe226c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_patches(patches):\n",
    "    '''Displays all patches in an image in their respective positions in the image.'''\n",
    "    n_rows, n_cols = patches.shape[0], patches.shape[1]\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*2, n_rows*2))\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            patch = patches[i, j, 0]  # shape: (patch_size, patch_size, 3)\n",
    "            axes[i, j].imshow(patch)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/',\n",
    "    read_images(voc_dir, False)[2][10] + '.jpg'\n",
    ")\n",
    "display_patches(patch_image(128, image))\n",
    "mask = os.path.join(\n",
    "    'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/',\n",
    "    read_images(voc_dir, False)[2][10] + '.png'\n",
    ")\n",
    "display_patches(patch_image(128, mask))\n",
    "\n",
    "patches1 = patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/SegmentationClass/2007_000033.png')\n",
    "display_patches(patches1)\n",
    "display_patches(patch_image(128, 'C:/Users/Hayden/Machine Learning/d2l/d2l-en/pytorch/chapter_computer-vision/data/VOCdevkit/VOC2012/JPEGImages/2007_000033.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64e8387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hayden\\AppData\\Local\\Temp\\ipykernel_24296\\221935971.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  return torch.tensor(images), torch.tensor(masks)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m320\u001b[39m, \u001b[38;5;241m480\u001b[39m)\n\u001b[0;32m     29\u001b[0m transforms \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[1;32m---> 30\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mVocDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoc_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m, in \u001b[0;36mVocDataset.__init__\u001b[1;34m(self, data_dir, transforms, is_train)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train \u001b[38;5;241m=\u001b[39m is_train\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m, in \u001b[0;36mVocDataset.load_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_images\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_patch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(image\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m))\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[0;32m     15\u001b[0m     ]\n\u001b[0;32m     16\u001b[0m     masks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(mask\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m masks\n\u001b[0;32m     19\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[23], line 31\u001b[0m, in \u001b[0;36mread_and_patch_images\u001b[1;34m(data_dir, is_train)\u001b[0m\n\u001b[0;32m     24\u001b[0m     images\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     25\u001b[0m         patch_image(\u001b[38;5;241m128\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPEGImages\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     26\u001b[0m     )\n\u001b[0;32m     27\u001b[0m     masks\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m     28\u001b[0m         patch_image(\u001b[38;5;241m128\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentationClass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     29\u001b[0m     )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(masks)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "class VocDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms, is_train):\n",
    "        self.data_dir = data_dir\n",
    "        self.filter = filter\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.images, self.masks = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        images, masks = read_and_patch_images(self.data_dir, self.is_train)\n",
    "        images = [\n",
    "            self.transforms(self.normalize(image.float() / 255))\n",
    "            for image in images\n",
    "        ]\n",
    "        masks = [\n",
    "            self.transforms(mask.float() / 255)\n",
    "            for mask in masks\n",
    "        ]\n",
    "        return images, masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.masks[index]\n",
    "\n",
    "filter = (320, 480)\n",
    "transforms = torchvision.transforms.ToTensor()\n",
    "train_set = VocDataset(voc_dir, transforms, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
